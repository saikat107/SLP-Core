This directory contains files and instructions to replicate FSE'17 results. These experiments were run with an older version of the code (v0.1), the Jar of which is included here. Several experiments can simply be replicated with the SLP_Core jar in the root of the repository; run `java -jar SLP-Core_v0.1.jar --help` to print its functions. Also see the Quick Start and (wiki) Getting Started guids for CLI usage, which has remained largely the same.

## Data
The data was downloaded from Allamanis et al.'s 2013 paper: Mining source code repositories at massive scale using language modeling ([link](http://groups.inf.ed.ac.uk/cup/javaGithub/)).
The names of the projects assigned to the 1% train/test/valid splits reported in the paper can be found in ProjectSplits.txt.
The IntsCreator.java file can be used to produce pre-translated files (and requires a vocabulary, which can be produced with the SLP-Core jar) from the three directories; translating all content to integers allows the models to be compared without needing a tokenizer

## Replicating experiment 1
Three smoothers (JM, WB and ADM) are built in to the jar and can be run with various orders by setting the `-o` option.

For the MKN smoother we used Tu et al.'s code as implemented at https://github.com/caseycas/CacheModelPackage.
Specifically, this code can only run 10-fold cross-validation. Thus, we ran IntsCreator.java on the corpus and copied the "ix-o-train" and "ix-o-test" file to a new folder, splitting the train file into 9 pieces.
Then, running 10-fold cross-validation yields one piece in which test is isolated from train, the scores of which we used

We slightly altered two things in this package:
- the file "evaluation/scripts/train.py", removing the "-unk" option from the SRILM parameters, and
- the file "evaluation/cachemodel/ngram.cpp", setting the OOV-entropy to 17.63 bits to match our vocabulary size (this model returns 10 bits of entropy by default for unseen events, which leads to invalid probability distributions).

## Replicating experiment 2
All the code for running various cache models in modeling and prediction setting is accessible through the API exposed by the jar.
The Jar supports a verbose mode which can be instructed to print all entropies and MRR scores (if one wishes to replicate the significance results) per file (which are stored by the model-runner).

## Replicating experiment 3
The LSTM configurations are documented in the paper; see the directory Tensorflow with four files which allow one to train (ptb_word_lm.py, uses reader.py) and write predictions (predictor.py, predictor-dynamic.py).
The reader expects files data/train, data/valid and data/test, which can be copied from the ix-o-\* files generated by the IntsCreator.java file from the 1% corpus.
The RNNLM implementation can be retrieved from Mikolov's website and runs quite easily using the configuration described by White et al.
Note that it only allows one to specify "direct connections" in terms of millions, where White et al. report using one thousand. A simple modification and re-compilation of Mikolov code takes care of this.
Note also that the MKN model that Mikolov mixes in may not work on this data due to discount errors; our ADM model pro-actively clips discount factors if they get too large or negative but the SRILM implementation appears negligent here.
In our paper, we did not mix Mikolov's RNN, although in our own experiments we have and have found that it is quite mutually beneficial to do so with JM-6.

Mixing the LSTM and n-gram results is the hardest part of all, the problem appearing to be with Tensorflow's code ocassionally skipping tokens.
The best way to go about it for us was to write all predictions and their probabilities to file in slp.core.modeling.MixModel.java, and then taking the parallel file generated by Tensorflow using predictory.py and aligning these.
Regarding the alignment: the best approach appears to be to skip a pair of lines when the models both believe they predicted correctly, but their predictions do not mutually agree.
Note that Tensorflow translates tokens using its own vocabulary internally, which must first be aligned. The mapping for this latter task, reconstructed to the best of our ability, is attached in Tensorflow/mapping

## Replicating experiment 4
The vocabulary experiments can again be replicated using the Jar.
The Tensorflow mixtures are slightly altered: simply do not credit the LSTMs with prediction points if they predict 0 (unknown token), setting the mixture to the n-gram's prediction if so.
For entropy, assign it the vocabulary base rate (log_2 |V|) in this setting. Note also that the n-gram may now predict many tokens which will all map to 0 for the LSTM, which from a mixture perspective is meaningless so do not go for the easy translation in this case.
